<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[hgcummings]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://hgc.io/"/>
  <updated>2014-08-27T22:03:16.806Z</updated>
  <id>http://hgc.io/</id>
  
  <author>
    <name><![CDATA[Harry Cummings]]></name>
    <email><![CDATA[hgcblog@electricweb.org.uk]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Continuous deployment to Heroku with Travis CI]]></title>
    <link href="http://hgc.io/2014/08/28/Continuous-deployment-to-Heroku-with-Travis-CI/"/>
    <id>http://hgc.io/2014/08/28/Continuous-deployment-to-Heroku-with-Travis-CI/</id>
    <published>2014-08-28T08:00:00.000Z</published>
    <updated>2014-08-27T22:03:16.806Z</updated>
    <content type="html"><![CDATA[<p>In <a href="/2014/08/26/Caching-Travis-CI-dependencies-in-S3/">the previous post</a> we discussed improving the speed of Travis CI builds by caching Maven dependencies in S3. In this post we’ll look at customising deployment to Heroku from Travis CI.</p>
<p>Travis CI supports <a href="http://docs.travis-ci.com/user/deployment/heroku/" target="_blank" rel="external">deployment to Heroku with very little config</a>. However, this approach wasn’t quite right for me for a couple of reasons:</p>
<ul>
<li>I wanted to deploy to Heroku during my build rather than at the end, so that I could run integration tests against my staging site</li>
<li>I wanted the option of pushing the deployment to live in the case where the build succeeded</li>
</ul>
<p>I also thought it might be possible to speed up the build a bit further by not re-building the application as part of the Heroku deploy, and not downloading the extra dependencies required by Travis CI’s Heroku deployment process.</p>
<h2 id="Requirements">Requirements</h2><p>Here’s the detailed order of events I want to happen when I push a new version of the code:</p>
<ul>
<li>Build the application and run unit tests</li>
<li>Deploy the application to Stage and run integration tests against it</li>
<li>Mark the build as a success or failure based on the test results</li>
<li>If the build is successful, then deploy the application to Live</li>
</ul>
<h2 id="The_Travis_CI_build_lifecycle">The Travis CI build lifecycle</h2><p>The <a href="http://docs.travis-ci.com/user/build-lifecycle/" target="_blank" rel="external">Travis CI build lifecycle</a> consists of the following steps:</p>
<ul>
<li><code>before_install</code></li>
<li><code>install</code></li>
<li><code>before_script</code></li>
<li><code>script</code></li>
<li><code>after_success</code> or <code>after_failure</code></li>
<li><code>after_script</code></li>
<li><code>deploy</code></li>
<li><code>after_deploy</code></li>
</ul>
<p>Each of these steps can be overridden by a custom script, except for the deploy step, which can only be configured to use one of Travis CI’s pre-defined deployment options.</p>
<p>To meet my requirements above, I’d need to deploy to Stage as part of the <code>script</code> step in order to pass or fail the build as a result, so can’t make use of the Heroku deployment feature provided by Travis CI. I could make use of this for deploying to Live, however having already built and deployed the application on Stage, it would be nice to push the built artifact to Live rather than having to build it again.</p>
<h2 id="Using_the_Heroku_API">Using the Heroku API</h2><p>Fortunately, Heroku support this exact scenario via its HTTP API. The process is as follows:</p>
<ul>
<li>Create a Heroku slug containing your application</li>
<li>Deploy that slug to stage</li>
<li>Deploy the same slug to Live</li>
</ul>
<div class="well"><small>It’s worth noting some of the reasons that git deployments are the standard for Heroku: Of course it ties in very nicely with the development workflow in some scenarios, but in my case I’m already kicking off the Travis CI build by a simple git push.<br>More significantly, it means that the application is built on the exact same environment that it will run, reducing the scope for odd environmental differences causing problems with the application. However, I’m happy that running my automated tests against the staging server would catch any problems like this.</small></div>

<h3 id="Creating_a_slug">Creating a slug</h3><p>A slug is just an archive containing your compiled application. Slugs are quite central to the Heroku build/deploy process but you don’t normally see them if using Heroku’s standard git-based deployment, since they are created and released automatically for you. Here’s what Heroku does for you when you push your code via git:</p>
<ul>
<li>Determines which build pack to use based on your project (e.g. the Java buildpack if you have a <code>pom.xml</code>)</li>
<li>Creates a slug using the appropriate buildpack</li>
<li>Deploys this slug to your application</li>
</ul>
<p>We’re kicking off our deployment from our CI server, so have already just compiled the application. This makes it easy to create a slug for our code. We just need a tar archive with all the contents under the <code>/app</code> path (which will be the base path of our application on the Heroku server):</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Preparing slug contents"</span></span><br><span class="line">mkdir app</span><br><span class="line">cp -r ./exwhy-web/target/classes ./app/classes</span><br><span class="line">cp -r ./exwhy-web/target/lib ./app/lib</span><br><span class="line"> </span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Creating slug archive"</span></span><br><span class="line">tar -czf slug.tgz ./app</span><br></pre></td></tr></table></figure>
<p>The next stage is to create a slug containing this archive. First we call the Heroku API to create a new slug. This returns an S3 URL to which we upload our archive. Lets look at the slug creation first:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Creating slug object"</span></span><br><span class="line">_heroku_deploy_apiKey=`<span class="built_in">echo</span> <span class="string">":<span class="variable">$&#123;HEROKU_API_KEY&#125;</span>"</span> | base64`</span><br><span class="line">_heroku_deploy_createSlugResponse=$(curl -X POST \</span><br><span class="line">-H <span class="string">"Content-Type: application/json"</span> \</span><br><span class="line">-H <span class="string">"Accept: application/vnd.heroku+json; version=3"</span> \</span><br><span class="line">-H <span class="string">"Authorization: <span class="variable">$&#123;_heroku_deploy_apiKey&#125;</span>"</span> \</span><br><span class="line"><span class="operator">-d</span> <span class="string">'&#123;"process_types":&#123;"web": "java $JAVA_OPTS -cp ./classes:./lib/* io.hgc.myapp.web.Application"&#125;&#125;'</span> \</span><br><span class="line">-n https://api.heroku.com/apps/<span class="variable">$&#123;HEROKU_STAGE&#125;</span>/slugs)</span><br></pre></td></tr></table></figure>
<p>We transform our API key into the right format and then make a <code>POST</code> to <code>https://api.heroku.com/apps/appName/slugs</code> to create a new slug. One thing to note here is request data (the <code>-d</code> parameter), specifying the web process. When using git-based deployment you would normally specify your processes in a Procfile. This file isn’t built into the slug but becomes part of its metadata, so when creating a slug from scratch we have to specify that metadata ourselves rather than using a Procfile.</p>
<p>The response from the above request will look something like the following:<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="string">"blob"</span>: &#123;</span><br><span class="line">    <span class="string">"method"</span>: <span class="string">"put"</span>,</span><br><span class="line">    <span class="string">"url"</span>: <span class="string">"https:\/\/s3-external-1.amazonaws.com\/herokuslugs\/heroku.com\/v1\/some-guid?credentials"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"buildpack_provided_description"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"commit"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"created_at"</span>: <span class="string">"2014-08-25T08:32:15Z"</span>,</span><br><span class="line">  <span class="string">"id"</span>: <span class="string">"9f6f132a-5a87-423b-b677-84c7a35a42c5"</span>,</span><br><span class="line">  <span class="string">"process_types"</span>: &#123;</span><br><span class="line">    <span class="string">"web"</span>: <span class="string">"java $JAVA_OPTS -cp .\/classes:.\/lib\/* io.hgc.myapp.web.Application"</span></span><br><span class="line">  &#125;,</span><br><span class="line">  <span class="string">"size"</span>: <span class="literal">null</span>,</span><br><span class="line">  <span class="string">"updated_at"</span>: <span class="string">"2014-08-25T08:32:15Z"</span>,</span><br><span class="line">  <span class="string">"stack"</span>: &#123;</span><br><span class="line">    <span class="string">"id"</span>: <span class="string">"7e04461d-ec81-4bdd-8b37-b69b320a9f83"</span>,</span><br><span class="line">    <span class="string">"name"</span>: <span class="string">"cedar"</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>The important things for us are the ID of the slug and the blob URL. First, we need to extract these from the response (<code>grep</code> is really not an appropriate tool for parsing JSON, but the response is very simple so we get away with it):<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> _heroku_deploy_parseField &#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="operator">-ne</span> <span class="variable">$2</span> | grep -o <span class="string">"\"<span class="variable">$1</span>\"\s*:\s*\"[^\"]*\""</span> | head -<span class="number">1</span> | cut <span class="operator">-d</span> <span class="string">'"'</span> <span class="operator">-f</span> <span class="number">4</span></span><br><span class="line">&#125;</span><br><span class="line">_heroku_deploy_blobUrl=$(_heroku_deploy_parseField <span class="string">"url"</span> <span class="string">"'<span class="variable">$&#123;_heroku_deploy_createSlugResponse&#125;</span>'"</span>)</span><br><span class="line">_heroku_deploy_blobHttpMethod=$(_heroku_deploy_parseField <span class="string">"method"</span> <span class="string">"'<span class="variable">$&#123;_heroku_deploy_createSlugResponse&#125;</span>'"</span>)</span><br><span class="line">_heroku_deploy_slugId=$(_heroku_deploy_parseField <span class="string">"id"</span> <span class="string">"'<span class="variable">$&#123;_heroku_deploy_createSlugResponse&#125;</span>'"</span>)</span><br></pre></td></tr></table></figure></p>
<p>Next we use the URL to upload our slug contents. The only wrinkle here is to make sure we put the HTTP method into uppercase, else the AWS signature check fails:<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"Uploading slug archive"</span></span><br><span class="line">curl -X <span class="variable">$&#123;_heroku_deploy_blobHttpMethod^^&#125;</span> -H <span class="string">"Content-Type:"</span> --data-binary @slug.tgz <span class="variable">$&#123;_heroku_deploy_blobUrl&#125;</span></span><br></pre></td></tr></table></figure></p>
<h3 id="Deploying_the_slug_to_stage">Deploying the slug to stage</h3><p>Finally, we create a new release of our application using the ID of our slug:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">function</span> deployToHeroku &#123; <span class="comment">#Args: application name</span></span><br><span class="line">    curl -X POST \</span><br><span class="line">    -H <span class="string">"Content-Type: application/json"</span> \</span><br><span class="line">    -H <span class="string">"Accept: application/vnd.heroku+json; version=3"</span> \</span><br><span class="line">    -H <span class="string">"Authorization: <span class="variable">$&#123;_heroku_deploy_apiKey&#125;</span>"</span> \</span><br><span class="line">    <span class="operator">-d</span> <span class="string">"&#123;\"slug\":\"<span class="variable">$&#123;_heroku_deploy_slugId&#125;</span>\"&#125;"</span> \</span><br><span class="line">    -n https://api.heroku.com/apps/<span class="variable">$1</span>/releases</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"Deploying slug to stage"</span></span><br><span class="line">deployToHeroku <span class="variable">$&#123;HEROKU_STAGE&#125;</span></span><br></pre></td></tr></table></figure>
<h3 id="Deploying_the_slug_to_Live">Deploying the slug to Live</h3><p>All we need to do to push the same slug to Live is to call <code>deployToHeroku</code> again with a different app name.</p>
<p>Putting this all together, we use the script in our .travis.yml file as follows:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">script: mvn clean install &#38;&#38; source ./build/heroku_deploy.sh &#38;&#38; mvn test -Dtest.server=http://$HEROKU_STAGE.herokuapp.com&#10;after_success: deployToHeroku $HEROKU_LIVE&#10;env:&#10;  global:&#10;  - HEROKU_STAGE: myapp-test&#10;  - HEROKU_LIVE: myapp&#10;  ...</span><br></pre></td></tr></table></figure>
<p>A couple of things to note here: Firstly, the <code>test.server</code> parameter is a custom parameter used by my test code, but you would probably want to do something similar if running tests against a staging server. Secondly, the various commands in the <code>script</code> step are all combined into one line with <code>&amp;&amp;</code>, rather than placed on separate lines. This is to make our build fail fast as soon as any one of them returns an error code, as described in the Travis CI documentation on <a href="http://docs.travis-ci.com/user/customizing-the-build/#Customizing-the-Build-Step" target="_blank" rel="external">customising the build</a>.</p>
<h2 id="Running_the_correct_Java_version">Running the correct Java version</h2><p>All of the above works perfectly, except for one thing: My application is built using Java 8, and the Heroku servers run Java 6 by default. When using Heroku’s git-based deployment, we would address this by adding a <code>system.properties</code> file in our project root containing <code>java.runtime.version=1.8</code>. However, like the ProcFile, this isn’t something that’s used at runtime but at build time in creating our slug.</p>
<p>I spent a while digging through the code of the <a href="https://github.com/heroku/heroku-buildpack-java/blob/master/bin/compile" target="_blank" rel="external">Heroku Java buildpack</a> and <a href="https://github.com/heroku/heroku-buildpack-jvm-common/blob/master/bin/java" target="_blank" rel="external">JVM common buildpack functions</a> to see how they worked. They actually download OpenJDK and include it in the slug archive. They’re also pretty complicated due to being quite general-purpose.</p>
<p>I could have imitated the relevant parts of these scripts making some simplifying assumptions, or downloaded and used the JVM common buildpack functions directly in my script. However, it occurred to me that there’s a much simpler way to get hold of a JRE of the right version for our application: We can just re-use the JDK from our build environment! All we need to do is add a couple of lines to our slug creation script:</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cp -r <span class="variable">$&#123;JAVA_HOME&#125;</span>/jre ./app/.jre</span><br><span class="line">mkdir <span class="string">"app/.profile.d"</span> &amp;&amp; <span class="built_in">echo</span> <span class="string">'export PATH="/app/.jre/bin:$PATH"'</span> &gt;&gt; app/.profile.d/java.sh</span><br></pre></td></tr></table></figure>
<p> The second line adds a script under the <code>.profile.d</code> folder, which means it will run on startup. All the script does is add the JRE’s bin folder to the front of the <code>$PATH</code> so that it gets used in preference to the JDK 1.6 installed on the system.</p>
<h2 id="Conclusion">Conclusion</h2><p>With the changes in this post and <a href="/2014/08/26/Caching-Travis-CI-dependencies-in-S3/">the previous one</a>, my build is now working exactly as I wanted, and much faster than before, down from its original time of 5-8 minutes to more like 1-2 minutes (usually closer to one minute). The most variable part of the build now is waiting for the new version of the application to start up on the staging server in order to run tests against it, which can take anything from ~20s to about a minute. However, the build doesn’t waste any time waiting for Heroku to build the application from scratch.</p>
<p>You can <a href="https://github.com/hgcummings/exwhy-spring/blob/master/build/heroku_deploy.sh" target="_blank" rel="external">find the complete and final script on GitHub</a> in the context of a demo Java application.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>In <a href="/2014/08/26/Caching-Travis-CI-dependencies-in-S3/">the previous post</a> we discussed improving the speed of Travis CI builds]]>
    </summary>
    
      <category term="bash" scheme="http://hgc.io/tags/bash/"/>
    
      <category term="heroku" scheme="http://hgc.io/tags/heroku/"/>
    
      <category term="java" scheme="http://hgc.io/tags/java/"/>
    
      <category term="maven" scheme="http://hgc.io/tags/maven/"/>
    
      <category term="travis-ci" scheme="http://hgc.io/tags/travis-ci/"/>
    
      <category term="Shell" scheme="http://hgc.io/categories/Shell/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Caching Travis CI dependencies in S3]]></title>
    <link href="http://hgc.io/2014/08/26/Caching-Travis-CI-dependencies-in-S3/"/>
    <id>http://hgc.io/2014/08/26/Caching-Travis-CI-dependencies-in-S3/</id>
    <published>2014-08-26T08:00:00.000Z</published>
    <updated>2014-08-27T22:00:20.937Z</updated>
    <content type="html"><![CDATA[<h2 id="The_problem">The problem</h2><p>I’m currently working on a small web application written in Java, using Travis CI as my build server. While the application itself is currently just a minimal “Hello World” Spring app, it already has quite a few dependencies. Downloading these was adding quite a lot of time to my CI builds. The problem was that since each build takes place in an isolated sandbox, Maven had to download all of my dependencies in each and every build.</p>
<p>To make matters worse, I’m also using Travis CI’s ability to deploy to Heroku. Heroku also builds the application from source and was also downloading all of the Maven dependencies every time. This meant that during each build, all of my dependencies were being downloaded <em>twice</em> and my build times were 5-8 minutes for a minimal “Hello World” app that didn’t even do anything yet.</p>
<h3 id="Alternative_solutions">Alternative solutions</h3><p>For private repositories, Travis CI supports <a href="http://docs.travis-ci.com/user/caching/" target="_blank" rel="external">caching dependencies between builds</a>. All you have to do to cache your Maven dependencies is add the following to the end of your .travis.yml file:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">cache:&#10;  directories:&#10;  - $HOME/.m2</span><br></pre></td></tr></table></figure>
<p>However, this doesn’t work for public repostories using Travis CI for free. Elsewhere in Travis CI’s documentation is an article on <a href="http://docs.travis-ci.com/user/speeding-up-the-build/" target="_blank" rel="external">speeding up the build</a>. This suggests rolling your own solution to cache dependencies in S3, or using <a href="https://github.com/Fingertips/WAD" target="_blank" rel="external">WAD</a> for Ruby projects managing dependencies via Bundler.</p>
<p>I considered trying to generalise WAD to allow it to cache dependencies other than Ruby gems, but concluded that it would be difficult to do this cleanly and that a custom build script would be more lightweight in the end.</p>
<h2 id="The_solution">The solution</h2><p>I wrote a small bash script to compress my local Maven folder and upload it to S3 at the end of each build, and to download and uncompress it at the start. This reduced my build time down to 2-3 minutes. The rest of this section explains how to use this script and a bit about how it works.</p>
<h3 id="Setting_up_an_S3_bucket">Setting up an S3 bucket</h3><p>The first step is to set up an S3 bucket and make it available to the build. Assuming you’re starting from scratch, you’ll need to do the following:</p>
<ul>
<li>Sign up for AWS</li>
<li>Create a bucket (I believe Travis CI is in ‘US Standard’ region, so it makes sense to use the same)</li>
<li>Create an IAM user, generating an access key and a secret (<em>keep note of these as AWS doesn’t let you to retrieve the secret again later on</em>)</li>
<li>Attach a policy based on the S3 Full Access template, replacing the resource string <code>*</code> with <code>arn:aws:s3:::bucketname/*</code></li>
</ul>
<p>Add <a href="http://docs.travis-ci.com/user/encryption-keys/" target="_blank" rel="external">encrypted environment variables</a> to your <code>.travis.yml</code> file for the credentials created above. Using <a href="https://github.com/travis-ci/travis.rb#readme" target="_blank" rel="external">the travis client</a>, just run:</p>
<pre><code>&gt; travis <span class="built_in">encrypt</span> AWS_ACCESS_KEY_ID=accesskey <span class="comment">--add</span>
&gt; travis <span class="built_in">encrypt</span> AWS_SECRET_ACCESS_KEY=secretkey <span class="comment">--add</span>
</code></pre><h3 id="Writing_the_build_script">Writing the build script</h3><p><a href="https://gist.github.com/hgcummings/c5be9de9cfe5ea416066" target="_blank" rel="external">The build script is available in full on Gist</a>. You can skip to <a href="#Using_the_build_script_in_-travis-yml">using the build script in .travis.yml</a> if you just want to get it working. The rest of this section goes into a bit more detail on how the script works internally. The beginning of the script looks like this:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="shebang">#!/bin/bash</span></span><br><span class="line">_s3_caching_dependencyFolder=<span class="variable">$HOME</span>/.m2</span><br><span class="line">_s3_caching_file=<span class="string">"cached.tar.bz2"</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">function</span> getCachedDependencies &#123;</span><br><span class="line">    <span class="keyword">if</span> [[ -z $(_s3_caching_diffPomFiles) ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"pom.xml files unchanged - using cached dependencies"</span></span><br><span class="line">        _s3_caching_downloadArchive</span><br><span class="line">        _s3_caching_extractDependencies</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">function</span> cacheDependencies &#123;</span><br><span class="line">    <span class="keyword">if</span> [[ -n $(_s3_caching_diffPomFiles) ]]; <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">"pom.xml files have changed - updating cached dependencies"</span></span><br><span class="line">        _s3_caching_compressDependencies</span><br><span class="line">        _s3_caching_uploadArchive</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"><span class="keyword">function</span> _s3_caching_diffPomFiles &#123;</span><br><span class="line">    git diff <span class="variable">$&#123;TRAVIS_COMMIT_RANGE&#125;</span> pom.xml **/pom.xml</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Implementation_details">Implementation details</h4><p>The extract/compress methods just call <code>tar</code>. The download/upload methods use <code>curl</code>, but are rather more involved as they have to construct a valid authentication header for the request to S3. I got on the right path thanks to <a href="http://tmont.com/blargh/2014/1/uploading-to-s3-in-bash" target="_blank" rel="external">Tommy Montgomery’s blog post on uploading to S3</a>. Unfortunately, Amazon have introduced a new authentication protocol since that post was written, and while I think the old one is still supported I couldn’t find it in the AWS documentation.</p>
<p>To support both uploads and downloads I ended up implementing <a href="http://docs.aws.amazon.com/AmazonS3/latest/API/sig-v4-header-based-auth.html" target="_blank" rel="external">the new authentication protocol</a> in my script. While quite fiddly to get working, the resulting script just uses <code>openssl</code> and a few other simple tools, all of which are available in the Travis CI build environment.</p>
<h4 id="Optimisation">Optimisation</h4><p>Finally, the conditionals and the <code>_s3_caching_diffPomFiles</code> function seen above form an optimisation to address a couple of issues:</p>
<ul>
<li>Compressing the dependencies and uploading them takes a while (~20s for me right now but this will grow over time)</li>
<li>When dependencies change, the old ones will continue to bloat our cache unless we recreate it from scratch</li>
</ul>
<p>To address both of these, I wanted to download cached dependencies only if they hadn’t changed since the last push, and update the cache only if they had changed (recreating it from scratch). I could do this by checking for any changes to the project’s <code>pom.xml</code> files. Fortunately, Travis CI provides an environment variable containing the commit range of the last push, which can be passed straight to <code>git diff</code> for this purpose.</p>
<h3 id="Using_the_build_script_in_-travis-yml">Using the build script in .travis.yml</h3><p>The build script can be used in .travis.yml as follows:</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">before_install: source ./build/s3_caching.sh&#10;install: getCachedDependencies&#10;after_script: cacheDependencies&#10;env:&#10;  global:&#10;  - AWS_BUCKET: bucketname&#10;  ...</span><br></pre></td></tr></table></figure>
<p><a href="http://docs.travis-ci.com/user/languages/java/" target="_blank" rel="external">Travis CI’s default behaviour for Java</a> projects is to call Maven twice: once in the install step to pull in dependencies and again in the script step to run the tests. The above config overrides the install step to use my script instead, and adds steps to load the script and cache dependencies after the build completes.</p>
<h2 id="Caching_dependencies_between_Heroku_builds">Caching dependencies between Heroku builds</h2><p>The above solution addresses the problem of Travis CI downloading all of my dependencies on every build. However, I mentioned at the start that Heroku was also downloading all of the dependencies for every deployment. <a href="https://devcenter.heroku.com/articles/java-support" target="_blank" rel="external">Heroku’s Java documentation</a> claims that it automatically caches dependencies between builds. Looking at the <a href="https://github.com/heroku/heroku-buildpack-java/blob/master/bin/compile" target="_blank" rel="external">source of the Heroku java buildpack</a> it’s fairly clear that it caches Maven dependencies.</p>
<p>Some further digging around in <a href="http://docs.travis-ci.com/user/deployment/heroku/#Deploy-Strategy" target="_blank" rel="external">the Travis CI documentation about Heroku</a> revealed that there are two deployment methods available: The standard git-based deployment and the alternative <em>Anvil</em>, which Travis CI uses by default. I didn’t spend a lot of time getting my head around Anvil, so I don’t know whether it simply doesn’t use the same buildpack or if there was some other issue, or how fixable it might be. However, there didn’t seem to be any particular downside to using git-based deployment. Instructing Travis CI to deploy to Heroku via git resulted in Maven dependencies being cached between Heroku builds as expected.</p>
<p>In <a href="/2014/08/28/Continuous-deployment-to-Heroku-with-Travis-CI/">the next post</a> I’ll cover another approach for deploying to Heroku and further optimising the build.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="The_problem">The problem</h2><p>I’m currently working on a small web application written in Java, using Travis CI as my build server]]>
    </summary>
    
      <category term="bash" scheme="http://hgc.io/tags/bash/"/>
    
      <category term="heroku" scheme="http://hgc.io/tags/heroku/"/>
    
      <category term="java" scheme="http://hgc.io/tags/java/"/>
    
      <category term="maven" scheme="http://hgc.io/tags/maven/"/>
    
      <category term="travis-ci" scheme="http://hgc.io/tags/travis-ci/"/>
    
      <category term="Shell" scheme="http://hgc.io/categories/Shell/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Procedural globe generation with d3js]]></title>
    <link href="http://hgc.io/2014/01/26/Procedural-globe-generation-with-d3js/"/>
    <id>http://hgc.io/2014/01/26/Procedural-globe-generation-with-d3js/</id>
    <published>2014-01-26T20:00:00.000Z</published>
    <updated>2014-04-20T22:22:58.102Z</updated>
    <content type="html"><![CDATA[<p>I recently entered the <a href="https://github.com/blog/1674-github-game-off-ii" target="_blank" rel="external">2013 GitHub Game Off</a> with a team of colleagues from <a href="http://www.softwire.com/" target="_blank" rel="external">Softwire</a>. You can see our project, including all of the code discussed below, <a href="https://github.com/Softwire/game-off-2013" target="_blank" rel="external">in our GitHub repository</a>.</p>
<p>The theme for the game off was ‘change’. After brainstorming a few ideas, we decided to build a game based around the idea of dealing with climate change. We split up development of the game amongst the team, and my main focus was on the GUI.</p>
<p>Our first prototype (an extremely minimal viable product) used some real world data to illustrate rising sea levels on a  world map. This was interesting in concept, however we decided it would be more appealing and better support our game mechanic if the interface was a more cartoonish fictional mini-planet instead.</p>
<a id="more"></a>
<p>I set out with an image in mind of a hex grid on a spinning globe. One important point was that our game would require interaction with the map, so I needed to be able to access and manipulate individual cells on the grid.</p>
<h2>Getting started</h2>

<div class="row"><div class="col-md-8">I thought this was a fairly common model so started looking for libraries that might help. I spent some time playing around with <a href="http://d3js.org/" target="_blank" rel="external">d3js</a>, which I’d already been using for the interface based on the real world, and which has some great features such as orthographic projection of co-ordinates. I also stumbled across a <a href="https://github.com/d3/d3-plugins/tree/master/geodesic" target="_blank" rel="external">d3js plugin for generating a geodesic sphere</a>, which was very close to what I wanted.<br><br>You can construct a geodesic sphere by starting with a regular <a href="http://en.wikipedia.org/wiki/Icosahedron" target="_blank" rel="external">icosahedron</a> (i.e. a D20), dividing its faces into smaller triangles, and then projecting those triangles out onto a sphere. If each edge is subdivided into <i>n</i> edges, then each face is subdivided into <i>n&sup2;</i> triangles. For <i>n=3</i> this results in the following shape:</div><div class="col-md-4 text-center"><a href="http://en.wikipedia.org/wiki/File:G%C3%A9ode_V_3_1.gif" target="_blank" rel="external"><img src="/images/20140126/Geode_V_3_1.gif"></a></div></div>

<h3>From triangles to hexagons (and a few pentagons)</h3>

<div class="row"><div class="col-md-8">It is, of course, impossible to tile a sphere using only hexagons, but you can get close by using 12 pentagons and a number of hexagons (which depends on the scale of the grid but can be arbitrarily large). You can construct such a grid by starting with the pattern of triangles described above and then constructing its <i>dual</i> by placing a vertex in the centre of each triangle. The next image shows the corresponding dual for the <i>n=3</i> geodesic sphere above.<br>While <a href="http://en.wikipedia.org/wiki/Dual_polyhedron" target="_blank" rel="external">dual polyhedra</a> are easy to describe in principal, actually constructing them turned out to be a bit tricky in practice. Each triangle in the initial grid would correspond to a vertex shared by three hexagons (or possibly a pentagon and two hexagons) in the dual. To construct each hexagon (or pentagon) in my grid, I would need to find the six (or five) corresponding triangles that would determine the vertices. These would involve determining neighbouring triangles.<br></div><br><div class="col-md-4 text-center"><a href="http://en.wikipedia.org/wiki/File:G%C3%A9ode_V_3_1_duale.gif" target="_blank" rel="external"><img src="/images/20140126/Geode_V_3_1_dual.gif"></a></div></div>

<p></p><h4>A flawed attempt</h4>My first approach relied on the fact that the grid of triangles was constructed in a predictable way. This meant I could just assign all of the triangles a numerical index and work out the patterns of indices that describe neighbouring triangles, which is actually possible generalise in terms of the parameter <i>n</i> mentioned above. The fact that the pattern of indices repeats for each face (modulo the number of triangles in a face) simplifies this a bit, although accounting for triangles that meet along the edges or at the vertices of the original icosahedron was messier. While I did get this approach to work, it resulted in some fairly ugly code that was also very difficult to test in any meaningful way.<p></p>
<div class="row"><div class="col-md-8"><h4>Keeping it simple</h4>I decided to go for another approach that would involve less clever code, but a bit more hard work for the computer. I found neighbours by brute force, taking each triangle in turn and searching through all the other triangles to find those that shared an edge. This resulted in much more understandable code that was also more general, which actually made it a little easier to test (by considering simpler cases). The cost of this generality was that the new code was definitely slower to run. However with some judicious choices about search order and a few easy optimisations I got it under a few seconds for the size of grid that I needed. At this point I was able to render my hex grid on a sphere, as in the following image.<br><br><em>Update:</em> I had a suspicion that the brute force part of this approach wasn’t necessary, but couldn’t quite see a way around it myself. However, I managed to get help with this particular problem in a <a href="http://www.reddit.com/r/gamedev/comments/1wf4b5/procedural_interactive_globe_generation_in/cf29kge?context=3" target="_blank" rel="external">discussion of this article on reddit</a>.<br><br></div><div class="col-md-4 text-center"><img class="img-rounded" src="/images/20140126/grid_blank.png"></div></div>

<h2>Terrain generation</h2>

<p>Once I had a hex grid I wanted to turn it into a little world, and that meant dividing it into continents and lakes, oceans and islands. Part of the mechanic of our game involved dealing with rising sea levels. This meant that I also needed each cell in the grid to have an altitude, so I could redraw the map if necessary as sea levels changed.</p>
<p>While writing the geodesic grid code had been a bit arduous, it yielded an extremely useful byproduct for this process: My algorithm for generating the hex grid made it very easy to store against each hexagon an array of its neighbours.</p>
<div class="row"><div class="col-md-8"><br>The algorithm I used for generating the land was as follows:<br><ul><li>Decide what the total number of land cells should be (e.g. 40% of the grid)</li><li>Set the starting altitude to the maximum desired</li><li>Keep choosing land cells completely at random until you reach a tenth of this number</li><li>Then choose land cells at random from the set of neighbours of existing land cells</li><li>Each time you pick a cell, set its altitude to the current altitude, and decrement the altitude</li></ul><br>This was just the first and simplest approach that came to mind, but it turned out to work quite well, producing pleasingly clumpy land masses with sufficiently interesting features. You can see the end result in the following image, and play around with it in <a href="http://softwire.github.io/game-off-2013/" target="_blank" rel="external">our prototype game</a> (use left/right arrow keys to spin the globe).<br><br></div><div class="col-md-4 text-center"><img class="img-rounded" src="/images/20140126/grid_terrain.png"></div></div>

<h2>A word on testing</h2>

<p>Unit testing in general and TDD in particular do not seem to be such mainstream practices in game development as they are in other parts of the software industry, possibly partly because they can be harder to apply. I was attempting to follow TDD on this project, but did find it quite challenging to write meaningful tests for the dual generation code. The second approach I took did enable me to write tests that covered all of the code, although I’m not convinced about how thoroughly my tests probe the output of the code for correctness, and I didn’t TDD it.</p>
<p>For the terrain generation on the other hand, I was pleasantly surprised. I expected procedural generation to be very difficult to test, but I actually ended up with a set of tests that nicely described the desired behaviour, and I was also able to stick to TDD, making one code change at a time to make the current test pass. The following tests (in order) drove the development of the whole algorithm:<br><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">it(<span class="string">'should mark the correct proportion of cells as land'</span>, <span class="function"><span class="keyword">function</span><span class="params">()</span> </span>&#123;...&#125;);</span><br><span class="line"></span><br><span class="line">it(<span class="string">'should mark the remaining cells as sea'</span>, <span class="function"><span class="keyword">function</span><span class="params">()</span> </span>&#123;...&#125;);</span><br><span class="line"></span><br><span class="line">it(<span class="string">'should clump together land locally'</span>, <span class="function"><span class="keyword">function</span><span class="params">()</span> </span>&#123;...&#125;);</span><br><span class="line"></span><br><span class="line">it(<span class="string">'should spread out land globally'</span>, <span class="function"><span class="keyword">function</span><span class="params">()</span> </span>&#123;...&#125;);</span><br></pre></td></tr></table></figure></p>
<p>One word of caution though: My procedural generation code does of course rely on a pseudo-random number generator in order to provide different terrain each time, and I initially accidentally wrote some non-deterministic unit tests, much to the consternation of the rest of the team! This was of course easily fixed by setting up a pRNG with a fixed seed value for the tests.</p>
<h2>Source code</h2>

<p>As mentioned at the top of the article, all of the source code is available on github. The most relevant parts for this article are:</p>
<ul>
<li>For the grid itself, <a href="https://github.com/Softwire/game-off-2013/blob/master/src/scripts/dual.js" target="_blank" rel="external">dual.js</a> (<a href="https://github.com/Softwire/game-off-2013/blob/master/test/dual.test.js" target="_blank" rel="external">tests</a>) and <a href="https://github.com/Softwire/game-off-2013/blob/master/src/scripts/grid.js" target="_blank" rel="external">grid.js</a> (<a href="https://github.com/Softwire/game-off-2013/blob/master/test/grid.test.js" target="_blank" rel="external">tests</a>)</li>
<li>For the terrain generation, <a href="https://github.com/Softwire/game-off-2013/blob/master/src/scripts/terrain.js" target="_blank" rel="external">terrain.js</a> (<a href="https://github.com/Softwire/game-off-2013/blob/master/test/terrain.test.js" target="_blank" rel="external">tests</a>)</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<p>I recently entered the <a href="https://github.com/blog/1674-github-game-off-ii">2013 GitHub Game Off</a> with a team of colleagues from <a href="http://www.softwire.com/">Softwire</a>. You can see our project, including all of the code discussed below, <a href="https://github.com/Softwire/game-off-2013">in our GitHub repository</a>.</p>
<p>The theme for the game off was ‘change’. After brainstorming a few ideas, we decided to build a game based around the idea of dealing with climate change. We split up development of the game amongst the team, and my main focus was on the GUI.</p>
<p>Our first prototype (an extremely minimal viable product) used some real world data to illustrate rising sea levels on a  world map. This was interesting in concept, however we decided it would be more appealing and better support our game mechanic if the interface was a more cartoonish fictional mini-planet instead.</p>]]>
    
    </summary>
    
      <category term="competitions" scheme="http://hgc.io/tags/competitions/"/>
    
      <category term="gamedev" scheme="http://hgc.io/tags/gamedev/"/>
    
      <category term="JavaScript" scheme="http://hgc.io/categories/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Js13kGames retrospective (part 2)]]></title>
    <link href="http://hgc.io/2013/09/30/Js13kgames-retrospective-part-2/"/>
    <id>http://hgc.io/2013/09/30/Js13kgames-retrospective-part-2/</id>
    <published>2013-09-30T20:00:00.000Z</published>
    <updated>2014-08-27T20:02:52.509Z</updated>
    <content type="html"><![CDATA[<p><a href="/2013/09/29/Js13kgames-retrospective-part-1/">Continuing from the previous post</a>, I’m looking at some of the lessons learned from <a href="http://js13kgames.com/entries/runrun-rabbit" target="_blank" rel="external">my entry in the Js13kGames coding competition</a>.</p>
<a id="more"></a>
<h3>The ugly</h3>

<h4>Concurrency</h4>

<p>Concurrency is hard, and brings with it the potential for whole classes of bugs you’ll never encounter in single-process systems, including devious Heisenbugs that might occur one time in a thousand. Getting a server and two (or more) clients to co-ordinate with each other and agree on consistent state is an order of magnitude more difficult than writing code that runs on a single machine.</p>
<p>I didn’t have too much of a problem with the typical multi-threading bugbear of shared mutable state, since my threads were necessarily isolated by running on different machines with minimal communication between them. However, getting them to all agree on the outcome of a pseudo-random quasi-chaotic user-interactive process (i.e. my game) was very hard indeed. Anything that made the system even slightly non-deterministic (e.g. timing differences, floating point errors, order of execution) could cause them to dramatically diverge. I spent a long time debugging these sorts of issues and fixing subtle bugs.</p>
<p>The final blow to this class of bugs was enforcing better separation of concerns in the clients, by having them update their state in lock-step with the server at a fixed time interval, and worrying about smoothing out the animations in the view (which calculated its own less precise but more frequently updated model of the current state). Everything suddenly became much easier after making this change, which was actually quite simple to do (although I’d missed the SoC opportunity before, I had at least kept my code reasonably modular).</p>
<p>The other big challenge was to make the server authoritative while allowing the clients to carry on the simulation for the mainline case where the server never tells them anything they disagree with (as discussed in <a href="http://www.gabrielgambetta.com/fpm1.html" target="_blank" rel="external">Gabriel Gambetta’s <em>Fast-paced Multiplayer</em> series</a>). Compensating for things working out differently (e.g. when two players perform conflicting actions and the server picks a winner) can be very complex and have a lot of knock-on effects. Only mini integration tests covering most of the shared model code made this tractable. I still have a few hypothetical scenarios to address on my to-do list though, which I believe will currently go wrong some tiny proportion of the time.</p>
<h4>That fox sprite</h4>

<p>I was rather pleased with the little purple bunnies that populated my game world, but really struggled to get the artwork for the foxes right. I had an idea that I wanted the fox sprite to be more angular than the rounded bunny sprite, so that it looked aggressive. However, it also needed to look larger than the rabbit sprite, which was already rather fat and round. Most of my attempts ended up looking a bit like a cat, or an enormous mouse, or simply no kind of animal at all.</p>
<p>Keeping in mind my issues with prioritisation (see above), I ended up settling for something that would do, and getting on with the rest of the game. However, the first thing I did after the competition was to redraw the fox sprite from scratch. You can compare the two versions below. I’m still not quite 100% pleased with the current version, but I’m much happier with how it fits in with the rest of the game’s artwork, and think it looks suitably fox-like. <div style="width: 640px; margin: auto"><h5>Before</h5><img src="/images/20130930/fox_old.png" class="img-rounded"><h5>After</h5><img src="/images/20130930/fox_new.png" class="img-rounded"></div></p>
<h3>What’s next</h3>

<p>I’ve carried on working on the game since the competition ended, and have started updating <a href="http://runrunrabbit.jit.su/" target="_blank" rel="external">the live deployed version</a> now that judging is over. Apart from updating the fox sprite, I’ve also…</p>
<ul>
<li>Fixed a few interface bugs raised by my colleagues at <a href="http://www.softwire.com" target="_blank" rel="external">Softwire</a> who were kind enough to playtest the game</li>
<li>Introduced more core gameplay features</li>
<li>Improved handling of network play edge-cases</li>
<li>Written a passable AI opponent for single play</li>
<li>Aided accessibility (in particular, colour-blind users)</li>
</ul>
<h4>Still to come</h4>

<p>There are a fair few things on my TODO list, including:</p>
<ul>
<li>Yet more concurrency edge cases (see above)</li>
<li>Four-player multiplayer</li>
<li>A lobby system and nicknames to allow people to co-ordinate games between friends</li>
<li>Improvements to the AI</li>
<li>Support for mobile/tablet platforms</li>
<li>More paranoid cheat prevention</li>
<li>Additional core gameplay features</li>
<li>Audio (SFX and music)</li>
</ul>
<p>Once I’ve worked through some of these, I also hope to market the game to some extent (possibly as part of the Ludum Dare <a href="http://www.ludumdare.com/compo/2013/09/27/october-challenge-2013/" target="_blank" rel="external">October Challenge</a>). I’ve really enjoyed taking part in this year’s Js13kGames and previous Ludum Dares, and - as brilliantly illustrated in <a href="http://www.mugshotgames.com/2013/08/publishing-your-first-game-in-40-not-so-easy-steps/" target="_blank" rel="external">this recent article by Steve Yap</a> - I suspect I’ve just scratched the surface of what you can learn from this kind of project.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><a href="/2013/09/29/Js13kgames-retrospective-part-1/">Continuing from the previous post</a>, I’m looking at some of the lessons learned from <a href="http://js13kgames.com/entries/runrun-rabbit">my entry in the Js13kGames coding competition</a>.</p>]]>
    
    </summary>
    
      <category term="competitions" scheme="http://hgc.io/tags/competitions/"/>
    
      <category term="gamedev" scheme="http://hgc.io/tags/gamedev/"/>
    
      <category term="General" scheme="http://hgc.io/categories/General/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Js13kGames retrospective (part 1)]]></title>
    <link href="http://hgc.io/2013/09/29/Js13kgames-retrospective-part-1/"/>
    <id>http://hgc.io/2013/09/29/Js13kgames-retrospective-part-1/</id>
    <published>2013-09-29T20:00:00.000Z</published>
    <updated>2014-08-27T18:44:38.925Z</updated>
    <content type="html"><![CDATA[<p>I recently entered <a href="http://js13kgames.com/" target="_blank" rel="external">Js13kGames</a>, a really well-organised JavaScript coding competition run by <a href="https://twitter.com/end3r" target="_blank" rel="external">Andrzej Mazur</a>. It’s a month-long game development competition with the key restriction that each game’s source code must fit within 13KB (when minified and zipped) and not pull in any external libraries or other resources at runtime.</p>
<p> I ended up taking 2nd place in the (admittedly not too hotly contested) server category. I learned a lot about JavaScript development, the HTML5 canvas API (which I’d never used before), as well as a few general lessons about software development…</p>
<a id="more"></a>
<h3>The good</h3>

<h4>Delivery</h4>

<p>It’s a common problem with software projects to not give sufficient thought to deployment until near the end of the project. It’s something that should be considered throughout the project (and ideally practiced throughout the project, via Continuous Deployment). By the time you get to the end it should be as close to a one-button process as possible.</p>
<p>The very first thing I did after deciding to enter the competition was to pester the organiser, Andrzej, with some questions about exactly how the game should be packaged for submission (the rules for the server category were slightly complicated). I then created a single grunt task that took care of everything (including bundling, minification, creating a server package with no dev dependencies, zipping up client and server code), and produced the exact things I needed to be able to submit the game:</p>
<ul>
<li>An output directory containing just what was needed to deploy the game, with a single command, to nodejitsu</li>
<li>Zipped client and server files for submission to the competition website (with the console output including the size of each file, so I could quickly check that I was still within the limit)</li>
</ul>
<p>Admittedly, the challenge was probably easier here than on most full-scale software projects, but the point still stands that it’s well-worth putting in the effort up-front to avoid a last-minute panic about delivery.</p>
<p>In the end I stayed up pretty late polishing my game entry, and was rather tired when it came to submitting it. Such was my state of mind that I managed to get the captcha on the submission form (which was something like “9 + 5 = ?”) wrong on my first attempt. It was a good job that I’d made sure it was pretty much impossible to get any other part of the submission process wrong.</p>
<h4>Testing</h4>

<p>TDD isn’t the first thing that springs to mind when you think of time-boxed coding competitions, and I can’t honestly say that I wrote the entire codebase test-first (or even test-last in places), but this approach was invaluable for parts of it…</p>
<p>Co-ordinating between shared models running on clients and a server is frightfully complex (see below), and it was only by sketching out the nasty edge cases and concocting tests for them that I had any hope of producing something robust. Having tests in place before trying to write the production code saved a lot of fumbling around in the dark.</p>
<p>The test coverage level across the codebase is somewhat inconsistent, but it’s close to 100% for all of the shared model code, and has been so since quite early on in the project. Working under tight time constraints for the competition made this testing if anything <em>more</em> worthwhile. As with packaging, having a one-button setup for running tests and static analysis (JSHint caught a few genuine bugs) was very helpful indeed.</p>
<h3>The bad</h3>

<h4>Time management</h4>

<p>I actually started writing this game as a <a href="http://www.ludumdare.com/" target="_blank" rel="external">Ludum Dare</a> entry (the weekend of the 27th Ludum Dare fell within the month that Js13kGames was running). I didn’t manage to finish something presentable in time for the 48-hour Ludum Dare competition. This was partly due to other time commitments, but also due to failing to prioritise work on the project well enough and getting distracted by things that weren’t on the critical path.</p>
<p>I actually didn’t find out about the Js13kGames until about halfway through its run, so was still a bit pressed for time. It was only by maintaining a backlog of features, shaping it regularly, and being disciplined about adding new ideas to the backlog rather than allowing them to sidetrack me, that I managed to finish something I felt was sufficiently presentable by the end of the competition.</p>
<h4>Abstraction</h4>

<p>One of the first things I had to do when re-purposing my game for Js13kGames rather than Ludum Dare was to tear out all of those pesky 3rd-party libraries. Getting rid of <a href="http://jquery.com/" target="_blank" rel="external">jQuery</a> and <a href="http://getbootstrap.com/" target="_blank" rel="external">Twitter Bootstrap</a> was frankly a bit of a chore, as they had been useful productivity boosters. However, removing the 2D graphics library, <a href="https://github.com/GoodBoyDigital/pixi.js" target="_blank" rel="external">pixi.js</a>, turned out to be a big benefit to the project.</p>
<p>This wasn’t because pixi.js is <em>worse</em> than jQuery or Twitter Bootstrap. Far from it: it’s a very nice little library. It was because I was using it for the wrong reasons: jQuery and Twitter Bootstrap were helpful because they allowed me to do work more quickly with technologies I already know well (JavaScript and the DOM, CSS and HTML). I wasn’t already familiar at all with HTML5 Canvas and had foolishly picked up a library instead of taking the time to learn a bit about the underlying technology.</p>
<p>As a result, although I familiarised myself with pixi.js’s API docs, I didn’t really understand what the library was doing, and therefore ended up doing some obviously dumb stuff (like effectively redrawing everything each frame). When I removed pixi.js, I spent a modest amount of time researching and reading a few great articles on Canvas (including those from <a href="https://developer.mozilla.org/en-US/docs/Web/Guide/HTML/Canvas_tutorial" target="_blank" rel="external">Mozilla</a>, <a href="http://www.html5rocks.com/en/tutorials/canvas/performance/" target="_blank" rel="external">HTML5 Rocks</a> and <a href="http://diveintohtml5.info/canvas.html" target="_blank" rel="external">Dive Into HTML5</a>), and ended up producing something much better.</p>
<p>Of course, if you do already have a good understanding of Canvas or WebGL, then pixi.js is a great library for working with these technologies and could save you a lot of time. I would probably use it again now that I know more.</p>
<p>This was an important reminder for me that you should typically know how things work one level of abstraction below where you’re operating (there’s a <a href="http://www.hanselman.com/blog/PleaseLearnToThinkAboutAbstractions.aspx" target="_blank" rel="external">great blog post on this by Scott Hanselmann</a>), and the highest level libraries that you’re using should only be a productivity tool rather than a crutch or an excuse for not understanding what’s going on at a slightly lower-level.</p>
<p></p><h3>The ugly</h3><a href="/2013/09/30/Js13kgames-retrospective-part-2/">Continued in the following post</a><p></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>I recently entered <a href="http://js13kgames.com/">Js13kGames</a>, a really well-organised JavaScript coding competition run by <a href="https://twitter.com/end3r">Andrzej Mazur</a>. It’s a month-long game development competition with the key restriction that each game’s source code must fit within 13KB (when minified and zipped) and not pull in any external libraries or other resources at runtime.</p>
<p> I ended up taking 2nd place in the (admittedly not too hotly contested) server category. I learned a lot about JavaScript development, the HTML5 canvas API (which I’d never used before), as well as a few general lessons about software development…</p>]]>
    
    </summary>
    
      <category term="competitions" scheme="http://hgc.io/tags/competitions/"/>
    
      <category term="gamedev" scheme="http://hgc.io/tags/gamedev/"/>
    
      <category term="General" scheme="http://hgc.io/categories/General/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[LiveReloading shared client- and server-side code]]></title>
    <link href="http://hgc.io/2013/08/31/LiveReloading-shared-client-and-server-side-code/"/>
    <id>http://hgc.io/2013/08/31/LiveReloading-shared-client-and-server-side-code/</id>
    <published>2013-08-31T22:00:00.000Z</published>
    <updated>2014-08-24T16:19:11.366Z</updated>
    <content type="html"><![CDATA[<h2 id="Background">Background</h2><h3 id="Motivation">Motivation</h3><p>I’m currently working on a simple network multiplayer game as a bit of a learning exercise. I’m using Javascript since the client is browser-based (I’ve been wanting to get some canvas/WebGL experience for ages) and I’ve found node.js to be great for small, focused server applications such as this.</p>
<p>Also, a multiplayer game is a really strong use case for sharing code between the client and server, as you need to run identical simulations on both (there are loads of great sources on this, but <a href="http://www.gabrielgambetta.com/fpm1.html" target="_blank" rel="external">Gabriel Gambetta’s <em>Fast-paced Multiplayer</em> series</a> is an excellent overview). This makes the combination of node.js and client-side JavaScript particularly appealing.</p>
<h3 id="Sharing_code">Sharing code</h3><p>I’m using <a href="http://browserify.org/" target="_blank" rel="external">browserify</a> to share code between the server and client, by allowing node-style CommonJS imports on the client side (as well as providing browser stand-ins for many the node.js core libraries). Getting this up-and-running with <a href="http://gruntjs.com/" target="_blank" rel="external">grunt</a> was very quick and easy.</p>
<p>Grunt is a node.js-based task runner (similar to <a href="https://github.com/mde/jake" target="_blank" rel="external">jake</a>, or Ruby’s <a href="http://rake.rubyforge.org/" target="_blank" rel="external">rake</a>), with lots of 3rd-party plugins for performing common tasks. To set up browserify with grunt, I just had to install <code>grunt-cli</code> and <code>grunt-browserify</code> via <code>npm</code>, and create a <code>Gruntfile.js</code> for my project like this:</p>
<pre><code>&gt; npm install -<span class="keyword">g</span> grunt-<span class="keyword">cli</span>
&gt; npm install grunt-browserify --<span class="keyword">save</span>-dev
</code></pre><h5 id="Gruntfile-js"><code>Gruntfile.js</code></h5><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = <span class="function"><span class="keyword">function</span><span class="params">(grunt)</span> </span>&#123;</span><br><span class="line">    grunt.initConfig(&#123;</span><br><span class="line">        clean: [ <span class="string">'build'</span> ],</span><br><span class="line">        browserify: &#123;</span><br><span class="line">            main: &#123;</span><br><span class="line">                src: [<span class="string">'client/main.js'</span>],</span><br><span class="line">                dest: <span class="string">'build/client/bundle.js'</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        copy: &#123;</span><br><span class="line">            main: &#123;</span><br><span class="line">                files: [</span><br><span class="line">                    &#123; src: [<span class="string">'client/static/*'</span>], dest: <span class="string">'build/client/'</span>, expand: <span class="literal">true</span>, flatten: <span class="literal">true</span> &#125;,</span><br><span class="line">                    &#123; src: [<span class="string">'client/lib/*'</span>], dest: <span class="string">'build/client/lib/'</span>, expand: <span class="literal">true</span>, flatten: <span class="literal">true</span> &#125;,</span><br><span class="line">                    &#123; src: [<span class="string">'server/**'</span>], dest: <span class="string">'build/'</span> &#125;,</span><br><span class="line">                    &#123; src: [<span class="string">'shared/**'</span>], dest: <span class="string">'build/'</span> &#125;</span><br><span class="line">                ]</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;);</span><br><span class="line"></span><br><span class="line">    grunt.loadNpmTasks(<span class="string">'grunt-browserify'</span>);</span><br><span class="line">    grunt.loadNpmTasks(<span class="string">'grunt-contrib-copy'</span>);</span><br><span class="line">    grunt.loadNpmTasks(<span class="string">'grunt-contrib-clean'</span>);</span><br><span class="line"></span><br><span class="line">    grunt.registerTask(<span class="string">'default'</span>, [<span class="string">'clean'</span>, <span class="string">'browserify'</span>, <span class="string">'copy'</span>]);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>I simply point browserify at a single bootstrapping client-side JavaScript file, and it recursively follows any <code>require</code> calls to pull in the whole set of necessary files, then packages them all into a single bundle.</p>
<p>I also added a grunt-contrib-copy task (to bring all the other files that aren’t part of the bundle across to the output directory), and a grunt-contrib-clean task (to wipe out the build directory before recreating it).</p>
<h2 id="The_problem">The problem</h2><p>The above worked great, but introduced a build step into my client-side code. Up until this point I’d been able to quickly see changes in the browser by refreshing, as I was keeping <a href="https://github.com/nodeapps/http-server" target="_blank" rel="external">http-server</a> running in my client source directory serving up the script files and other static assets.</p>
<p>Now, I had to stop the http-server (to release the lock on the output directory), run grunt (to create the browserify script bundle) and restart the server. This is only a few quick commands, but made a big difference when I was doing things like obsessively tweaking my rendering logic for drawing cartoon purple bunny rabbits. Also, as I began to work on more server-side code I’d need to restart my actual application server anyway, not just a dumb static asset http-server.</p>
<h2 id="The_solution">The solution</h2><p>I do really like node.js and the whole Javascript ecosystem. However, since node.js and <a href="https://npmjs.org/" target="_blank" rel="external">npm</a> appeared, there’s been an overwhelming proliferation of Javascript libraries and tools, many of them individually evolving at breakneck speed, and it’s often difficult to know where to begin.</p>
<p>In the end, I needed to make use of the following:</p>
<ul>
<li><a href="https://github.com/gruntjs/grunt-contrib-watch" target="_blank" rel="external">grunt-contrib-watch</a> for re-running the build process whenever my files changed</li>
<li><a href="https://github.com/remy/nodemon" target="_blank" rel="external">nodemon</a> for running and restarting the server (I assume it’s pronounced node-daemon rather than node-mon)</li>
<li><a href="http://livereload.com/" target="_blank" rel="external">LiveReload</a> for getting the browser to automatically refresh (actually an improvement on the original situation)</li>
</ul>
<h3 id="Putting_it_all_together">Putting it all together</h3><h4 id="Install">Install</h4><pre><code>&gt; npm install grunt-contrib-watch --<span class="built_in">save</span>-<span class="built_in">dev</span>
&gt; npm install grunt-nodemon --<span class="built_in">save</span>-<span class="built_in">dev</span>
&gt; npm install grunt-concurrent --<span class="built_in">save</span>-<span class="built_in">dev</span>
</code></pre><p><a href="https://github.com/sindresorhus/grunt-concurrent" target="_blank" rel="external">grunt-concurrent</a> is an extra bit of glue allowing me to run watch and nodemon simultaneously.</p>
<p>Also install <a href="http://feedback.livereload.com/knowledgebase/articles/86242-how-do-i-install-and-use-the-browser-extensions-" target="_blank" rel="external">the relevant browser extension for livereload</a>.</p>
<h4 id="Add_grunt_tasks">Add grunt tasks</h4><figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">grunt.loadNpmTasks(<span class="string">'grunt-contrib-watch'</span>);</span><br><span class="line">grunt.loadNpmTasks(<span class="string">'grunt-nodemon'</span>);</span><br><span class="line">grunt.loadNpmTasks(<span class="string">'grunt-concurrent'</span>);</span><br></pre></td></tr></table></figure>
<h4 id="Configure_grunt_tasks">Configure grunt tasks</h4><ul><li>First <code>watch</code>, to run the browserify bundling process (and copy the output files to the right place)</li><li>Then <code>nodemon</code>, to run my server start script<ul><li>Note that I’ve actually configured nodemon to monitor the build directory, so it’s effectively downstream of the browserify step</li><li>There are various other options for choosing which files to monitor or exclude, but this was the neatest one for me and avoided any duplication with the watch step</li><li>The delayTime makes nodemon wait a moment after first spotting a change in case there are any other changes very shortly after</li></ul></li><li>Finally <code>concurrent</code>, to run both processes and combine their command-line output</li></ul>

<figure class="highlight javascript"><table><tr><td class="code"><pre><span class="line">watch: &#123;</span><br><span class="line">    files: [<span class="string">'client/**/*.js'</span>, <span class="string">'shared/**/*.js'</span>, <span class="string">'client/**/*.html'</span>, <span class="string">'client/**/*.css'</span>],</span><br><span class="line">    tasks: [<span class="string">'browserify'</span>, <span class="string">'copy'</span>],</span><br><span class="line">    options: &#123;</span><br><span class="line">        livereload: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line">nodemon: &#123;</span><br><span class="line">    dev: &#123;</span><br><span class="line">        options: &#123;</span><br><span class="line">            file: <span class="string">'start.js'</span>,</span><br><span class="line">            watchedFolders: [<span class="string">'build'</span>],</span><br><span class="line">            delayTime: <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;,</span><br><span class="line">concurrent: &#123;</span><br><span class="line">    target: &#123;</span><br><span class="line">        tasks: [<span class="string">'nodemon'</span>, <span class="string">'watch'</span>],</span><br><span class="line">        options: &#123;</span><br><span class="line">            logConcurrentOutput: <span class="literal">true</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>So now, whenever I make a change to client- or server-side files, I just have to wait a second before the server restarts and the browser automatically refreshes. Perfect!</p>
<p>If only I’d managed to sort this out <em>before</em> last weekend’s <a href="http://www.ludumdare.com/compo/" target="_blank" rel="external">Ludum Dare</a> I might have ended up submitting an entry. Unfortunately I ran out of time (not <em>just</em> due to faffing with client/server build process). I’m very pleased to have got it working since though, and have carried on development of the game that I started over the Ludum Dare weekend. You can view the working result of all of the above in <a href="https://github.com/hgcummings/ld-27" target="_blank" rel="external">the project’s github repo</a>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Background">Background</h2><h3 id="Motivation">Motivation</h3><p>I’m currently working on a simple network multiplayer game as a bit]]>
    </summary>
    
      <category term="browserify" scheme="http://hgc.io/tags/browserify/"/>
    
      <category term="grunt" scheme="http://hgc.io/tags/grunt/"/>
    
      <category term="livereload" scheme="http://hgc.io/tags/livereload/"/>
    
      <category term="node.js" scheme="http://hgc.io/tags/node-js/"/>
    
      <category term="nodemon" scheme="http://hgc.io/tags/nodemon/"/>
    
      <category term="JavaScript" scheme="http://hgc.io/categories/JavaScript/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Code generation in .NET with Roslyn (part 3)]]></title>
    <link href="http://hgc.io/2012/07/12/Code-generation-in-net-with-roslyn-part-3/"/>
    <id>http://hgc.io/2012/07/12/Code-generation-in-net-with-roslyn-part-3/</id>
    <published>2012-07-12T08:57:00.000Z</published>
    <updated>2014-08-24T20:39:31.233Z</updated>
    <content type="html"><![CDATA[<p>This post is the third in a series on code generation on the .NET platform. In this post, we will look at how to package a code generator as a Visual Studio extension, and how best to share a single library between multiple extensions.<br><a id="more"></a></p>
<h3 id="Creating_a_single_file_code_generator">Creating a single file code generator</h3><p>Visual Studio allows you to hook into code generators from within the IDE by associating files in your project with a ‘Custom tool’ that produces another file based on the first, which will automatically be included in the project as well. The original file might be a class file, XML (e.g. a dataset definition), or any other kind of project file. The output file will typically be some source code, and might define a new class, or contain a partial class definition for the same class as in the original file.</p>
<p>Creating a single file code generator actually involves quite a lot of boilerplate, including various COM interop assemblies, several poorly documented interfaces, and some tedious steps to add your extension to the registry so Visual Studio will pick it up. Fortunately, there’s <a href="http://code.msdn.microsoft.com/SingleFileGenerator" target="_blank" rel="external">an MSDN sample that covers all of this for you</a> and includes an example code generator for you to crib from if you get stuck. The base classes implement all the necessary boilerplate and just leave you to override a single, fairly self-explanatory, abstract method:</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">protected</span> <span class="keyword">abstract</span> <span class="keyword">byte</span>[] GenerateCode(<span class="keyword">string</span> inputFileContent);</span><br></pre></td></tr></table></figure>
<p>This is all very nice, apart from the fact that the documentation says “To build and execute the sample, press F5 after the sample is loaded. This will launch the experimental hive which will demonstrate the sample’s function.” (The words “experimental hive” just means it makes a new place in the registry for all the settings used by the test instance of Visual Studio, so you can muck about with installing extensions without having any wider effect). This didn’t work at all for me, and despite finding <a href="http://hocke.blogspot.com/2010/09/how-to-get-single-file-generator-sample.html" target="_blank" rel="external">a blog post claiming to get it working in Visual Studio 2010</a> I couldn’t get to a point where I could develop my extension and just hit F5 to debug it in Visual Studio. Trying to get it to work involved digging into a lot of the work the sample is supposed to abstract away in the first place, and ended up being a bit of a time sink for me.</p>
<p>However, it’s not actually that important to be able to debug your extension in Visual Studio. In fact, if you’re just writing a code generator it’s pretty pointless. Every time you debug, you would have to create or open a project in the ‘debuggee’ Visual Studio instance and set your code generator as the custom tool on a file. It’s much quicker to just write a unit test for your code generator with a suitable input string (either read in from a file or just specified as a constant in the unit test code), and debug via this test instead. Once you’re happy with the behaviour of your code generator under test, it’s very easy to install your extension in Visual Studio to check that it still works (just double-click the .vsix file in your output directory and open a new instance of VS).</p>
<h3 id="Sharing_a_library_between_extensions">Sharing a library between extensions</h3><p>Having created a Roslyn-based code generator, I wanted to create a shared library of any code that wasn’t specific to my code generation scenario but could be used by any Roslyn-based code generator. This would include some of the extension methods I’d written, and a base class for abstracting away some of the more general communication with Roslyn.</p>
<p>For VS 2010, extensions are packaged into VSIX files, which follow a fairly neatly-specified XML-based format used in the nice Extensions ‘gallery’ introduced in the latest version of Visual Studio. Unfortunately, packaging an extension containing multiple assemblies doesn’t ‘just work’ as one might hope it would. I also experimented with creating a ‘base extension’, but this is rather more complex and wasn’t necessary for my purposes. There’s a fairly thorough <a href="http://blogs.msdn.com/b/visualstudio/archive/2010/06/09/vsix-best-practices.aspx" target="_blank" rel="external">VSIX Best Practices blog post</a> explaining the various approaches and when you should use each. It mentions packaging multiple assemblies into one extension, but doesn’t quite go into detail on how to achieve this.</p>
<p>It turns out that packaging multiple assemblies into a Visual Studio extension is actually very simple if you edit the VSIX manifest directly, but the Visual Studio ‘Designer’ for this file (as is the case with many Visual Studio ‘Designer’ tools) is a bit half-baked and doesn’t understand the complete spec of the file type it’s supposed to edit. The designer does however helpfully support some much more esoteric options that are explicitly ruled out by the official source on best practices (see the ‘Things to Avoid’ section in the blog post mentioned above).</p>
<p>You can find the final VSIX manifest I created, along with the rest of the source for this blog series, in my <a href="https://github.com/hgcummings/RoslynGenerators" target="_blank" rel="external">RoslynGenerators repository on github</a>. This repository includes a project template and a supporting library for creating your own Roslyn-based single file code generators as Visual Studio extensions. The repository also includes an example generator for creating an asynchronous version of a WCF service interface.</p>
<h3 id="Postscript:_Not_forgetting…">Postscript: Not forgetting…</h3><p>While this series was primarily an explanation of what’s possible with Roslyn, I couldn’t conclude without mentioning a few other existing tools and libraries.</p>
<h4 id="Existing_third-party_libraries">Existing third-party libraries</h4><p>It’s worth noting some excellent third-party libraries that cover much of the same functionality as promised by Roslyn. In particular, <a href="https://github.com/icsharpcode/NRefactory/" target="_blank" rel="external">NRefactory</a> is a C#/VB parser (developed for the third-party SharpDevelop IDE) that does almost exactly what’s described in the section “Introducing Roslyn” in the first post of this series, while the Mono project’s <a href="http://docs.go-mono.com/index.aspx?link=N%3AMono.CSharp" target="_blank" rel="external">CSharp</a> namespace covers some of the other features available in Roslyn (e.g. compiler-as-a-service, and a REPL).</p>
<h4 id="T4">T4</h4><p>T4 is the Text Template Transformation Toolkit, which is part of Visual Studio and therefore the standard tool for code generation in .NET. If you’re already familiar with T4, you might have been wondering why it hasn’t come up in this series so far. The reason is that T4 is a much more than just another code generation process. It’s a very general purpose tool for generating code (or indeed any text-based output). T4 templates can call arbitrary assemblies, so could make use of any of the options discussed in the first post of this series (although T4 naturally pushes you towards using plain text for your output model).</p>
<p>In fact, I could have written a T4 template that made use of Roslyn for code generation, but I went with writing a Visual Studio extension because it felt like a better fit. In particular, while T4 templates can be very powerful and I’ve found them useful in the past, the template code itself can become quite difficult to maintain. I prefer to write my code generation logic in a normal average .NET project; benefiting from Visual Studio’s standard syntax highlighting, intellisense, and many useful design- and compile-time checks.</p>
<p>I certainly don’t mean to dismiss T4 entirely. In particular, there are some 3rd-party Visual Studio plugins that make T4 templates much easier to work with, by providing syntax highlighting and intellisense among other features: <a href="http://visualstudiogallery.msdn.microsoft.com/60297607-5fd4-4da4-97e1-3715e90c1a23" target="_blank" rel="external">Tangible T4 Editor</a> and <a href="http://visualstudiogallery.msdn.microsoft.com/40a887aa-f3be-40ec-a85d-37044b239591" target="_blank" rel="external">Clarius Visual T4</a> both offer similar feature sets and have a similar business model (a limited free version and a full-featured ‘pro’ version). Additionally, T4 is highly extensible; you can create your own text templating host and directive processors to control how your templates are compiled and run. For an excellent overview of the T4 architecture and many more T4 resources, see <a href="http://www.olegsych.com/2008/05/t4-architecture/" target="_blank" rel="external">Oleg Sych’s blog</a>.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>This post is the third in a series on code generation on the .NET platform. In this post, we will look at how to package a code generator as a Visual Studio extension, and how best to share a single library between multiple extensions.<br>]]>
    
    </summary>
    
      <category term="c#" scheme="http://hgc.io/tags/c/"/>
    
      <category term="metaprogramming" scheme="http://hgc.io/tags/metaprogramming/"/>
    
      <category term="roslyn" scheme="http://hgc.io/tags/roslyn/"/>
    
      <category term=".NET" scheme="http://hgc.io/categories/NET/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Code generation in .NET with Roslyn (part 2)]]></title>
    <link href="http://hgc.io/2012/07/05/Code-generation-in-net-with-roslyn-part-2/"/>
    <id>http://hgc.io/2012/07/05/Code-generation-in-net-with-roslyn-part-2/</id>
    <published>2012-07-05T18:32:00.000Z</published>
    <updated>2014-08-24T20:39:34.285Z</updated>
    <content type="html"><![CDATA[<p>This post is the second in a series on code generation on the .NET platform. In this post, we will take a closer look at Microsoft’s <a href="http://msdn.microsoft.com/en-gb/roslyn" target="_blank" rel="external">Community Technology Preview of ‘Roslyn’</a>. In brief, this is a C# compiler implemented in managed code that exposes an API to let you hook into the compilation process.</p>
<h3 id="Recap">Recap</h3><p>In the previous post, I introduced Roslyn as an interesting tool for code generation, in particular when generating source code from other source code. Roslyn is especially appealing for this purpose because it provides a strongly-typed model for working with source code and (unlike most code generation approaches) allows you to use the same model for both input and output.</p>
<p>One of the common drawbacks of most other options for code generation (as discussed in the previous post) is the need to translate from one model to another and how clumsy this can be, particularly when you want to carry across some elements of the input source code without changing them. A detailed example of when you might need to do this is discussed in the appendix at the end of this post.<a id="more"></a></p>
<p>This kind of code generation should be easier to implement using Roslyn, since using the same model for input and output doesn’t force you to create your output code from scratch, but instead allows you to generate a modified version of your input code. The overall outline of the process is as follows:</p>
<ul>
<li>Read in the source code of the original contract and get Roslyn to parse it into a strongly-typed syntax tree</li>
<li>Manipulate the bits of the syntax tree that we’re interested in, leaving the rest alone</li>
<li>Get Roslyn to write out code for the modified syntax tree</li>
</ul>
<p>The rest of this post covers what I learned as I went about implementing a Roslyn-based code generator in this way.</p>
<h3 id="Working_with_Roslyn">Working with Roslyn</h3><p>The current Roslyn release has some limitations (there’s a <a href="http://social.msdn.microsoft.com/Forums/en-US/roslyn/thread/f5adeaf0-49d0-42dc-861b-0f6ffd731825" target="_blank" rel="external">complete list of known issues on the MSDN forums</a>, which has been updated for the second CTP). However the API is supposed to be fairly stable, so anything learnt about it or written against it now should remain relevant for the final release. I found this to be the case when updating from the first CTP to the second. I did have to make some changes, but in most cases it was very easy to see how to translate my code to use the new API. None of the changes forced me to re-order things or otherwise alter the structure of my program; it was mostly just a matter of using slightly different methods and updating some property names.</p>
<h4 id="Creating_Syntax_Trees_&amp;_Nodes">Creating Syntax Trees &amp; Nodes</h4><p>You can parse a string into a syntax tree using the SyntaxTree.ParseCompilationUnit method. The API of Roslyn’s syntax tree model is reasonably discoverable. <strong>All the various syntax objects are created by static factory methods</strong>, which you can reach by typing ‘<code>Syntax.</code>‘ and following Intellisense. The Factory methods have lots of parameters, but make heavy use of named parameters with default values, so you can get away without specifying all of them. However, you need to be careful about this…</p>
<p>One thing you’ll notice quite quickly is that <strong>the syntax tree model is designed to be able to represent broken code</strong>. This makes sense, since one of the intended uses of Roslyn is writing code fix-up tools like ReSharper. This does mean that if you want to transform from valid code to valid code, there’s quite a bit of additional complexity in the API that you might prefer not to have to think about (e.g. having to specify that you’d like separators between your parameters, or semi-colons at the end of you interface method signatures, when anything else wouldn’t compile anyway). When you create a new syntax node without specifying some property of it, the API seemed to be slightly inconsistent in whether the default behaviour will be the most sensible one (i.e. whatever will compile).</p>
<h4 id="Manipulating_the_model">Manipulating the model</h4><p><strong>Syntax trees and the nodes within them are immutable</strong>. Most syntax nodes have an Update method which doesn’t affect the original node, but returns a new one with your changes applied. For example, the original Roslyn documentation suggested something like this for renaming a namespace:</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">NamespaceDeclarationSyntax newNamespace =</span><br><span class="line">    oldNamespace.Update(oldNamespace.NamespaceKeyword,</span><br><span class="line">                        newNamespaceName,</span><br><span class="line">                        oldNamespace.OpenBraceToken,</span><br><span class="line">                        oldNamespace.Externs,</span><br><span class="line">                        oldNamespace.Usings,</span><br><span class="line">                        oldNamespace.Members,</span><br><span class="line">                        oldNamespace.CloseBraceToken,</span><br><span class="line">                        oldNamespace.SemicolonTokenOpt);</span><br></pre></td></tr></table></figure>
<p>For some reason, these Update methods don’t make the same use of named parameters and default values that the factory methods do. This means that if you just want to change one property they’re rather pointlessly verbose (as in the above example). I previously wrote my own extension methods for this purpose, so the above would become:</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 'Update' in this case is an extension method</span></span><br><span class="line">NamespaceDeclarationSyntax newNamespace =</span><br><span class="line">    oldNamespace.Update(name: newNamespaceName);</span><br></pre></td></tr></table></figure>
<p>However, with the second Roslyn CTP in June, Microsoft have addressed this in a slightly different way by adding lots of fluent-style methods for updating one property at a time, so the above would become:</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">NamespaceDeclarationSyntax newNamespace = oldNamespace.WithName(newNamespaceName);</span><br></pre></td></tr></table></figure>
<p>You can create a new updated tree by calling Roslyn’s ReplaceNodes() extension method on the root node (which again doesn’t modify the original tree but returns a new tree with the specified nodes replaced), passing it the nodes you want to replace and a function for transforming each node. The signature for ReplaceNodes() looks like this:</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">static</span> TRoot ReplaceNodes&lt;TRoot, TNode&gt;(</span><br><span class="line">        <span class="keyword">this</span> TRoot root,</span><br><span class="line">        IEnumerable&lt;TNode&gt; oldNodes,</span><br><span class="line">        Func&lt;TNode, TNode, SyntaxNode&gt; computeReplacementNode)</span><br><span class="line">    <span class="keyword">where</span> TRoot : SyntaxNode <span class="keyword">where</span> TNode : SyntaxNode;</span><br></pre></td></tr></table></figure>
<p>It’s not immediately obvious why the computeReplacementNode function type has <strong>two</strong> input parameters of type TNode. In all the cases where I was using this method, Roslyn always passed in the exact same (i.e. reference equal) object for both. Fortunately, the second CTP has <a href="http://social.msdn.microsoft.com/Forums/en-US/roslyn/thread/91613dc0-290e-408c-b136-2d1d4a23e295" target="_blank" rel="external">an extremely helpful FAQ with extensive code samples in the form of unit tests</a>, which includes an example covering this method. It turns out that the first parameter represents the node before any replacements have been made, and the second parameter represents the original node with its descendants already replaced. The two arguments may often be the same if you’re only replacing one or two specific nodes, rather than performing a more general transformation on most or all of the nodes in the tree.</p>
<p>If you want to add or remove nodes of a tree (rather than just updating them), this typically becomes a matter of updating a list property on the relevant parent node, again either using Update() or one of the new fluent-style “With-“ methods.</p>
<h4 id="Outputting_source_code">Outputting source code</h4><p>Finally, you can get back from a source tree to a string by calling GetText() on your new root node. However, you’ll definitely want to call Format() on the root node before you do so: This method is responsible for making sure the outputted code has sensible and consistent whitespace. Beware that this is more than just a matter of making your code look pretty: If you don’t call Format, Roslyn might miss out vital things like spaces between parameter types and parameter names (think about it for a second…)! Note that you only need to call Format() on the outermost node: Calling it on inner nodes first makes no difference (and presumably takes some amount of processing).</p>
<p>With the second CTP, the formatting methods have been moved out into the Roslyn.Services assembly, and the API has changed slightly so that Format() returns an IFormattingResult object rather than a new root node. This interface includes a new GetTextChanges method, which may be useful in some scenarios. However, if you just want the complete text for the newly formatted node though, you can use the method chain <code>.Format().GetFormattedRoot().GetText();</code></p>
<h4 id="Gotchas">Gotchas</h4><p>I did encounter a few cases where I had to use the debugger to explore the original syntax tree and work out what was going on (although, a couple of samples included with the CTP are <a href="http://blogs.msdn.com/b/visualstudio/archive/2011/10/19/roslyn-syntax-visualizers.aspx" target="_blank" rel="external">syntax visualization tools</a>, which may also be useful for this purpose). For example: Many SyntaxNode types have an Attributes property of type SyntaxList, which makes sense. What didn’t make sense to me initially was that each AttributeDeclarationSyntax in this list itself has a property called Attributes of type SeparatedSyntaxList. I eventually realised this was due to an esoteric feature of the C# language that allows you to add attributes to a class using a comma-seperated <code>[Attribute1, Attribute2, Attribute3]</code> syntax, rather than the more usual practice of putting each attribute in it’s own pair of square brackets on a new line. I ended up just using <code>attributeList.SelectMany(a =&gt; a.Attributes)</code>.</p>
<p>This is a good example of how Roslyn’s source code model differs significantly from the Reflection model (which is only concerned with compiled types, where the two cases above would appear identical) or even CodeDOM (which is a model of code rather than of compiled types, but is too general to ever differentiate between such minor syntax variations). This clearly illustrates how Roslyn gives you more control than something like CodeDOM, but at the cost of some additional complexity you have to bear in mind.</p>
<h3 id="Wrapping_up">Wrapping up</h3><p>Having successfully got Roslyn doing what I wanted it to, I thought it would be worth packaging up my Roslyn-based code generator into a neat Visual Studio extension because, after all, how hard could it be? We’ll find out in the next post…</p>
<h3 id="Appendix:_A_real-world_problem">Appendix: A real-world problem</h3><p>The particular proof-of-concept I chose to implement for this exercise was a generator for creating an asynchronous WCF service interface from a synchronous version. This is a common requirement in order to make use of some clever WCF plumbing that allows you to implement a service contract consisting of simple, synchronous methods, and to consume that service via a contract containing corresponding asynchronous methods (which may be more appropriate for some clients or communication scenarios). An example of what these two contracts might look like follows:</p>
<figure class="highlight csharp"><table><tr><td class="code"><pre><span class="line">[ServiceContract]</span><br><span class="line">[XmlSerializerFormat]</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title">IService1</span></span><br><span class="line">&#123;</span><br><span class="line">  [OperationContract]</span><br><span class="line">  <span class="function"><span class="keyword">string</span> <span class="title">GetData</span><span class="params">(<span class="keyword">int</span> <span class="keyword">value</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">  [OperationContract]</span><br><span class="line">  <span class="function">OutputType <span class="title">GetComplexData</span><span class="params">(InputType <span class="keyword">value</span>)</span></span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[ServiceContract]</span><br><span class="line">[XmlSerializerFormat]</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">interface</span> <span class="title">IService1Async</span></span><br><span class="line">&#123;</span><br><span class="line">  [OperationContract(AsyncPattern = <span class="keyword">true</span>)]</span><br><span class="line">  <span class="function">IAsyncResult <span class="title">BeginGetData</span><span class="params">(<span class="keyword">int</span> <span class="keyword">value</span>, AsyncCallback callback, <span class="keyword">object</span> state)</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">string</span> <span class="title">EndGetData</span><span class="params">(IAsyncResult result)</span></span>;</span><br><span class="line"></span><br><span class="line">  [OperationContract(AsyncPattern = <span class="keyword">true</span>)]</span><br><span class="line">  <span class="function">IAsyncResult <span class="title">BeginGetComplexData</span><span class="params">(InputType <span class="keyword">value</span>,</span><br><span class="line">                                   AsyncCallback callback, <span class="keyword">object</span> state)</span></span>;</span><br><span class="line">  <span class="function">OutputType <span class="title">EndGetComplexData</span><span class="params">(IAsyncResult result)</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Even if you’re not familiar with WCF, you can see that the second interface contract is a lot more complex than the first and may be correspondingly more difficult to implement. You can also see that translating the first interface contract into the second is just a matter of following some simple rules and could be quite a mechanical process.</p>
<p><hr></p>
<h4 id="Aside:">Aside:</h4><p>In fact, this translation can be performed by svcutil.exe, a program that ships with the .NET framework, the main purpose of which is to generate code for WCF service clients. However, if the service and client are part of the same project and the same codebase, you don’t need to generate contracts on the client side (it’s also quite difficult to integrate svcutil into your build process if you want the client to automatically pick up changes to the service, since it requires the service code to be not only compiled but deployed and running). For these reasons a lot of people put their service contracts in a shared assembly and reference it directly from the client. However, this means you also lose the benefit of svcutil creating asynchronous versions of your service contracts.<hr><br>What would be nice is to get an asynchronous version of your WCF contract that automatically stayed up-to-date at design time (without having to build and run your service). I’ve come across a few attempts to do this which have forgotten to carry across something from the original definition to the generated one (e.g. the XmlSerializerFormatAttribute on the contracts above), or have included a lot of complex code to handle all the fiddly edge-cases you have to worry about (e.g. writing out attribute parameters correctly) when attempting to faithfully reproduce all of these aspects of the original code. What you really want is a code generator that</p>
<ul>
<li>Transforms each service method according to the rules for turning synchronous methods into asynchronous ones</li>
<li>Doesn’t alter any other part of the code, but automatically reproduces it in the generated contract</li>
</ul>
<p>Writing a code generator using Roslyn allowed me to focus on the first point, while achieving the second point without even having to think about it.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>This post is the second in a series on code generation on the .NET platform. In this post, we will take a closer look at Microsoft’s <a href="http://msdn.microsoft.com/en-gb/roslyn">Community Technology Preview of ‘Roslyn’</a>. In brief, this is a C# compiler implemented in managed code that exposes an API to let you hook into the compilation process.</p>
<h3 id="Recap">Recap</h3><p>In the previous post, I introduced Roslyn as an interesting tool for code generation, in particular when generating source code from other source code. Roslyn is especially appealing for this purpose because it provides a strongly-typed model for working with source code and (unlike most code generation approaches) allows you to use the same model for both input and output.</p>
<p>One of the common drawbacks of most other options for code generation (as discussed in the previous post) is the need to translate from one model to another and how clumsy this can be, particularly when you want to carry across some elements of the input source code without changing them. A detailed example of when you might need to do this is discussed in the appendix at the end of this post.]]>
    
    </summary>
    
      <category term="c#" scheme="http://hgc.io/tags/c/"/>
    
      <category term="metaprogramming" scheme="http://hgc.io/tags/metaprogramming/"/>
    
      <category term="roslyn" scheme="http://hgc.io/tags/roslyn/"/>
    
      <category term=".NET" scheme="http://hgc.io/categories/NET/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Code generation in .NET with Roslyn (part 1)]]></title>
    <link href="http://hgc.io/2012/06/28/Code-generation-in-net-with-roslyn-part-1/"/>
    <id>http://hgc.io/2012/06/28/Code-generation-in-net-with-roslyn-part-1/</id>
    <published>2012-06-28T13:49:00.000Z</published>
    <updated>2014-04-19T08:09:03.727Z</updated>
    <content type="html"><![CDATA[<h3 id="Introduction">Introduction</h3><p>This post is the first in a series on code generation on the .NET platform. This is a huge subject area and there are a bewildering number of relevant libraries and tools that are useful for this purpose. I will make an effort to mention the most important third-party options (particularly where they’ve pre-empted Microsoft’s efforts), and may revisit some of them in future posts. However, for this series I will focus on those libraries and tools that are part of the .NET framework itself, or will be included in future versions. In particular, I’ll explore building code generation tools using Microsoft’s in-development compiler-as-a-service project, code-named “<a href="http://msdn.microsoft.com/en-gb/roslyn" target="_blank" rel="external">Roslyn</a>“.</p>
<p>In this post I’ll briefly discuss what we mean by code generation, before going through some of the current tools and libraries for code generation in .NET, along with their shortcomings. I’ll also introduce Roslyn and explain why it initially caught my interest as a potentially useful library for code generation. Subsequent posts in this series will cover what I learned about using Roslyn and creating Visual Studio extensions, by looking at a specific WCF-related code generation scenario.<br><a id="more"></a></p>
<h4 id="What_is_Roslyn?">What is Roslyn?</h4><p>Roslyn is the codename for a Microsoft project to develop a C# compiler implemented in managed (i.e. .NET) code, which exposes an API allowing you to hook in to various steps of the compilation process. CTP stands for Community Technology Preview (Microsoft’s terminology for a beta version): This is an opportunity for developers to get a sneak preview of the features that will be available in future releases of .NET &amp; Visual Studio, and for Microsoft to gather early feedback on the direction they’re taking. The initial CTP was released back in October. Since then, Microsoft have be incorporating feedback and adding new functionality, culminating in a <a href="http://blogs.msdn.com/b/jasonz/archive/2012/06/05/announcing-microsoft-roslyn-june-2012-ctp.aspx" target="_blank" rel="external">second CTP released this month</a>.</p>
<h4 id="What_is_code_generation?">What is code generation?</h4><p>Code generation, at least for the purposes of this series, is any automated process for writing source code. It isn’t what happens at compile-time (which is more to do with making human-readable code machine-readable) or dynamic meta-programming at runtime (e.g. using reflection), and happens at a stage before either of these, sometimes referred to as design-time. In fact, if you’re a .NET programmer you’ve probably already used code generation in one way or another if you’ve used any of the Visual Studio ‘designer’ tools (such as those for web forms, XML datasets or Entity Framework models). These tools</p>
<h3 id="Choosing_a_code_generation_process">Choosing a code generation process</h3><p>When performing code generation, you will always have: Some input, a model for working with that input, and a model for generating your output code. The input for code generation can be almost anything, but is often either XML (e.g. a WSDL service definition or a dataset schema) or some existing .NET types (anything from an individual interface declaration to an entire class library). It’s the latter case that I’m particularly interested in and will be considering for the rest of this section.</p>
<h4 id="Choosing_an_input_model">Choosing an input model</h4><p>The first decision is whether you want the input to your code generator to be raw source code, or a compiled assembly. The latter allows you to make use of powerful (and perhaps more familiar) APIs to work with the input, such as the .NET framework’s System.Reflection namespace. The main downside to this approach is that your code generator can’t run until the source types have been compiled, which can make it difficult to fit the required code generation step into your build process. This can be particularly problematic if the types that you’re outputting are closely related to the input types (which will often be the case) and conceptually belong in the same library.</p>
<p>If you take source code as your input model then the options are rather limited. In some very specific cases it might make sense to work on source code as if it’s plain text, but this is obviously not feasible for anything other than the most trivial input code. Another option is <a href="http://msdn.microsoft.com/en-us/library/envdte(v=VS.100).aspx" target="_blank" rel="external">EnvDTE</a>, an assembly-wrapped COM library for Visual Studio automation that allows you to access Visual Studio’s model of the source code in your project. This is a very powerful approach, but COM automation libraries are often a bit painful to work with.</p>
<h4 id="Choosing_an_output_model">Choosing an output model</h4><p>The options here are also fairly limited. The simplest and most obvious option is to treat your generated code as just a bunch of text, writing to a StringBuilder or TextWriter. This is a more reasonable approach than in the case of the input model, but it’s still error-prone and the lack of strong-typing can make your code generation logic difficult to maintain. There’s a .NET library that attempts to address these issues called CodeDOM.</p>
<h4 id="CodeDOM">CodeDOM</h4><p>.NET includes the CodeDOM namespace for building and manipulating source code through a strongly-typed Document Object Model (much as you might manipulate an HTML DOM using Javascript). The CodeDOM API is reasonably discoverable and contains classes to represent everything from whole assemblies (i.e. <a href="http://msdn.microsoft.com/en-us/library/system.codedom.codecompileunit.aspx" target="_blank" rel="external">CodeCompileUnit</a>) down to individual expressions (e.g. <a href="http://msdn.microsoft.com/en-us/library/system.codedom.codeobjectcreateexpression.aspx" target="_blank" rel="external">CodeObjectCreateExpression</a>, <a href="http://msdn.microsoft.com/en-us/library/system.codedom.codevariablereferenceexpression.aspx" target="_blank" rel="external">CodeVariableReferenceExpression</a>). It’s also quite well documented on MSDN (see <a href="http://msdn.microsoft.com/en-us/library/y2k85ax6.aspx" target="_blank" rel="external">using the CodeDOM</a> for a good starting point). It works well for generating code with lots of structure and not much logic (making it quite suitable for the example discussed above of generating classes for an XSD schema).</p>
<p>Some drawbacks of CodeDOM are:</p>
<ul>
<li>It’s extremely verbose (as you might expect) and while quite good for generating structure it can be pretty painful for generating logic</li>
<li>It hasn’t really moved with the times and doesn’t have any way of expressing some newer language features (it hasn’t seen any significant updates since .NET 2.0 and the corresponding versions of C# and VB)</li>
<li>It’s intentionally language agnostic, meaning it doesn’t have a strongly-typed way of expressing some of the more useful language-specific features, although it also has some bizarre omissions like unary operators (e.g. boolean inversion), breaks, continues, switches, while loops etc. (see <a href="http://blogs.msdn.com/b/bclteam/archive/2005/03/16/396915.aspx" target="_blank" rel="external">Language Features which can’t be expressed using CodeDOM</a> for a list of common grievances)</li>
</ul>
<p>Where you need to output something CodeDOM has no way of expressing, you can use a <a href="http://msdn.microsoft.com/en-us/library/system.codedom.codesnippetexpression.aspx" target="_blank" rel="external">CodeSnippetExpression</a>, which just takes a string that will be output exactly as provided. This obviously ties your generated CodeDOM model to a specific language, which is unlikely to be a problem for a given project. However, if you used CodeSnippetExpressions extensively, you also lose most of the benefits of CodeDOM over a plain text model.</p>
<p>There are a few third-party projects that attempt to make CodeDOM a bit nicer to work with, by providing more succinct wrappers around the API. The most promising of these is <a href="http://www.codeproject.com/KB/cs/refly.aspx" target="_blank" rel="external">Refly</a>. <a href="http://linq2codedom.codeplex.com/" target="_blank" rel="external">Expressions to CodeDOM</a> looks interesting too, although I haven’t used it myself.</p>
<h4 id="Introducing_Roslyn">Introducing Roslyn</h4><p>We will discuss Roslyn in much more detail in the next post, but I’ll briefly explain why it’s interesting for code generation. As a compiler it obviously includes a parser, and the API allows you to to generate a ‘syntax tree’ from your source code (a syntax tree being a strongly-typed structure of objects representing your source code). You can also go in the opposite direction, generating source code from a syntax tree that you have built (either from scratch or, more likely, based on another tree you generated from source code). This is particularly appealing as it allows you to work with the same, strongly-typed, model for both input and output. This is something that isn’t quite possible with any of the options discussed above.</p>
<p>In the following post, I’ll discuss how I got on with using Roslyn to implement a fairly basic code generation scenario.</p>
]]></content>
    <summary type="html">
    <![CDATA[<h3 id="Introduction">Introduction</h3><p>This post is the first in a series on code generation on the .NET platform. This is a huge subject area and there are a bewildering number of relevant libraries and tools that are useful for this purpose. I will make an effort to mention the most important third-party options (particularly where they’ve pre-empted Microsoft’s efforts), and may revisit some of them in future posts. However, for this series I will focus on those libraries and tools that are part of the .NET framework itself, or will be included in future versions. In particular, I’ll explore building code generation tools using Microsoft’s in-development compiler-as-a-service project, code-named “<a href="http://msdn.microsoft.com/en-gb/roslyn">Roslyn</a>“.</p>
<p>In this post I’ll briefly discuss what we mean by code generation, before going through some of the current tools and libraries for code generation in .NET, along with their shortcomings. I’ll also introduce Roslyn and explain why it initially caught my interest as a potentially useful library for code generation. Subsequent posts in this series will cover what I learned about using Roslyn and creating Visual Studio extensions, by looking at a specific WCF-related code generation scenario.<br>]]>
    
    </summary>
    
      <category term="c#" scheme="http://hgc.io/tags/c/"/>
    
      <category term="metaprogramming" scheme="http://hgc.io/tags/metaprogramming/"/>
    
      <category term="roslyn" scheme="http://hgc.io/tags/roslyn/"/>
    
      <category term=".NET" scheme="http://hgc.io/categories/NET/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Getting on with eBooks]]></title>
    <link href="http://hgc.io/2012/03/02/Getting-on-with-ebooks/"/>
    <id>http://hgc.io/2012/03/02/Getting-on-with-ebooks/</id>
    <published>2012-03-02T00:02:00.000Z</published>
    <updated>2014-04-19T08:09:03.725Z</updated>
    <content type="html"><![CDATA[<p>Since sustaining an actual injury while jogging into work with a copy of an optimistically-titled ‘… in a Nutshell’ book in my backpack, I decided to explore the world of electronic books, and have been pleasantly surprised by what I’ve found. For a number of reasons I think it’s a really great medium for technical content. I thought the following points were worth sharing, and might dispel one or two popular misconceptions about eBooks…</p>
<a id="more"></a>
<h3>It’s actually fine</h3>

<p>I was very sceptical of electronic books for a very long time, and resisted even trying them out until last summer, when I went abroad on a hand-luggage-only ticket and wanted to take several 800-odd page books with me to revise for looming MCPD exams. I gave in, borrowed a Kindle, and made room to pack my sandals…</p>
<p>It turned out that using a Kindle was OK and even enjoyable. I still prefer paper books, and I haven’t been able to bring myself to read any novels on an eBook reader just yet (sticking to good old-fashioned dead tree for now), but it is mostly a matter of just getting used to it. I slightly miss being able to easily flick to another point in the book, or glance at how many pages are left in the current chapter. But apart from these very minor niggles, reading technical books electronically works very well, and I quickly forget about the medium once I start reading. In fact, I’ve started to prefer the PDF versions of technical books, and in the last few months I’ve only come across a single book (<a href="http://artofgamedesign.com/" target="_blank" rel="external">The Art of Game Design</a>) where I thought the print version was so well presented that I actually preferred it to the electronic format.</p>
<h3>Impulse purchases are good for your education</h3>

<p>The ease with which very cheap (or free) eBooks can be acquired means that I can keep my eBook reader topped up with good content, while being able to carry it with me everywhere means that I get through quite a lot of reading without having to specially set aside much time for it. My train journey to work is only ten minutes, but together with other travel time in the evenings and at weekends, this actually adds up to a decent amount of reading time each week. I’ve found I’ve done a lot more reading recently just by merit of having a device with a steady stream of books on it with me at all times.</p>
<p>When I find out about a new project that I’ll be working on, I usually look up a relevant book or three to read. I also keep a general eye out for cheap deals and good free content. Examples include <a href="http://www.apress.com/" target="_blank" rel="external">Apress</a>‘s $10 Deal of the Day, daily and weekly deals from <a href="http://shop.oreilly.com/" target="_blank" rel="external">O’Reilly</a>, magazine resources such as <a href="http://www.developerdotstar.com" target="_blank" rel="external">developer.*</a>, excellent free eBooks like <a href="http://progit.org/" target="_blank" rel="external">Pro Git</a>, and medium-length content such as <a href="http://www.infoq.com/minibooks/" target="_blank" rel="external">InfoQ’s minibooks</a>.</p>
<p>Other advantages include always being able to get the latest version of a books (which is obviously particularly relevant to technical publishing). For example, I picked up <a href="http://pragprog.com/book/rails4/agile-web-development-with-rails" target="_blank" rel="external">the eBook version of Agile Development with Ruby on Rails</a>, which was updated just last month for Rails 3.2 (an updated print version isn’t available yet).</p>
<h3>For technical eBooks: Amazon is irrelevant, and DRM is of no concern</h3>

<p>Amazon is usually our first port of call for technical books in print, and it’s obviously hugely dominant in the eBook market in general. But for technical eBooks it’s (perhaps surprisingly) a bit pointless; as are other online stores such as Kobo. On these sites, eBooks tend to be slightly cheaper than their print counterparts, but not by a huge amount. Also, they tend to sell books in specific formats that aren’t well suited for technical books (see below), and they typically impose fairly restrictive DRM on books (although this varies quite a bit, even from one title to another). Indeed, DRM is another concern that put me off eBooks for quite a while.</p>
<p>However, all of the major technical publishers sell their eBooks direct, without any DRM, in multiple formats, often a bit cheaper than Amazon. Furthermore, the negligible marginal cost of eBooks means they regularly offer deals allowing you to pick up electronic versions of books for a fraction of the print price (as mentioned above). Most (if not all) publishers digitally watermark the books in some way (so you can’t just put them on The Pirate Bay), which seems fair enough, but the lack of restrictions means you don’t have to worry about difficulty transferring books between your own devices.</p>
<p>The technical books market seems to have converged on this as the standard way of doing business, and every technical publisher I could think of sells their books on very similar terms, including:</p>
<ul>
<li><a href="http://shop.oreilly.com/category/customer-service/ebooks.do" target="_blank" rel="external">The one with line drawings of animals</a> (who also distribute the Microsoft Press titles under the same terms)</li>
<li><a href="http://www.manning.com/about/ebooks.html" target="_blank" rel="external">The one with paintings of Eastern European folk dancers</a></li>
<li><a href="http://www.apress.com/customer-support/" target="_blank" rel="external">The black and yellow one</a></li>
<li><a href="http://pragprog.com/frequently-asked-questions/ebooks" target="_blank" rel="external">The trendy one</a></li>
<li><a href="http://www.packtpub.com/" target="_blank" rel="external">Packt</a></li>
</ul>
<p>It’s also worth noting that any content updates publishers make to their electronic offerings take a little while to filter through to the versions sold through other distributors (e.g. Amazon’s Kindle edition etc.). Also, regarding DRM, note that although there’s no technical restriction on <em>lending</em> books to other people, some publishers do explicitly disallow this in their licensing terms.</p>
<h3>PDFs are not going away anytime soon</h3>

<p>In the general eBook market, there’s been a big move towards new formats that are well suited to reading novels on small screens. However, the most common standard and proprietary eBook formats (e.g. ePub and mobi) just don’t work that well for technical books, where layout is extremely important and re-flowing text just doesn’t work. This is mainly due to diagrams and source code listings, although technical books also make quite heavy use of other formatting (tables, lists, callouts, various levels of subheadings etc.). There’s a bit of variation between publishers in how much care they put into various eBook formats, but in almost all cases you should just head straight for the PDF.</p>
<h3>You don’t need an eBook reader to read eBooks</h3>

<p>Most eBook readers have a 6” screen, which isn’t very practical for technical books. Your best bet in this case is to turn your eBook reader sideways and display half a page at a time in landscape mode. A 9” eBook reader is capable of displaying a full page at a time at a comfortable zoom level, but there aren’t many of these around. Amazon seem to be quietly letting the Kindle DX die, and the only two alternatives on the market right now are produced by relative minnows Pocketbook and Onyx.</p>
<p>That said, I took the plunge and bought an Onyx M92 when it was first released just before Christmas. I’m happy with it; the build quality of the device seems very high, and I’ve found the firmware to be slightly unpolished but very stable. However, I’m not convinced it’s worth about 80% of the price of an iPad 2.</p>
<p>I think the value of e-Ink devices in general is pretty debatable. You can pick up a 10” Android tablet for around £150, which can do a heck of a lot more than just read books. A 9” eBook reader is a single-purpose device (despite the best efforts of manufacturers to find something else to do with them), which will set you back by about twice as much. The only really big advantage is battery life: For e-Ink devices, this is typically measured in weeks or even months, compared to days or hours for backlit tablets &amp; smartphones.</p>
<p>The important point is that you definitely don’t need to shell out £300-odd for an eBook reader if you’re not sure that you’ll find it that useful, and in fact many of you probably already own a portable device that would be just fine for reading PDFs on the train, even if you hadn’t thought of using it for that before now.</p>
<h3>And finally… eBooks will not save the world</h3>

<p>The marketing material for Kindles and other eBook readers point out how wonderfully green you’ll be now that you’re not killing trees anymore every time you buy a book. The heft of some programming language reference volumes might indicate that this argument carries a little more weight for people who read technical books. However, I’m sceptical about how many eBooks I’ll need to read instead of paper ones to offset the environmental cost of manufacturing my extremely intricate electronic reader and shipping it from China.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Since sustaining an actual injury while jogging into work with a copy of an optimistically-titled ‘… in a Nutshell’ book in my backpack, I decided to explore the world of electronic books, and have been pleasantly surprised by what I’ve found. For a number of reasons I think it’s a really great medium for technical content. I thought the following points were worth sharing, and might dispel one or two popular misconceptions about eBooks…</p>]]>
    
    </summary>
    
      <category term="books" scheme="http://hgc.io/tags/books/"/>
    
      <category term="publishing" scheme="http://hgc.io/tags/publishing/"/>
    
      <category term="General" scheme="http://hgc.io/categories/General/"/>
    
  </entry>
  
</feed>